{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Recommended tools:\n",
    "- Python 3.11+\n",
    "- VSCode\n",
    "- Data Wrangler - to explare data in output\n",
    "- nbstripout - to automatically omit jupiter notebook output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 1. Import and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Count missing values and attach their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame({'missing_count':missing_values})\n",
    "missing_df['dtype'] = train[missing_df.index].dtypes\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Visualise missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,10))\n",
    "sns.heatmap(train.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title(\"Missing values in train.csv\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 2. Process missing values (mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "I looked into data_description.txt and found values that we can substitute for the NaN values for the categorical data.\n",
    "Then added manually missing_value_fill_type as the last column in the dataframe:\n",
    "```\n",
    "              missing_count    dtype missing_value_fill_type\n",
    "PoolQC                 1453   object    NA\n",
    "MiscFeature            1406   object    NA\n",
    "Alley                  1369   object    NA\n",
    "Fence                  1179   object    NA\n",
    "MasVnrType              872   object    None\n",
    "FireplaceQu             690   object    NA\n",
    "LotFrontage             259  float64\n",
    "GarageType               81   object    NA\n",
    "GarageYrBlt              81  float64\n",
    "GarageFinish             81   object    NA\n",
    "GarageQual               81   object    NA\n",
    "GarageCond               81   object    NA\n",
    "BsmtExposure             38   object    NA\n",
    "BsmtFinType2             38   object    NA\n",
    "BsmtQual                 37   object    NA\n",
    "BsmtCond                 37   object    NA\n",
    "BsmtFinType1             37   object    NA\n",
    "MasVnrArea                8  float64\n",
    "Electrical                1   object    # Not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Create df that will represent data described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with default values ​​for columns with gaps.\n",
    "# There can be special default values like 'median_by_neighborhood' - they will have specific processing rules in process missing values function.\n",
    "# Here we only specify special cases; the rest will be handled automatically (like 0 for int/float values).\n",
    "default_fill = {\n",
    "    'PoolQC': 'NA',\n",
    "    'MiscFeature': 'NA',\n",
    "    'Alley': 'NA',\n",
    "    'Fence': 'NA',\n",
    "    'MasVnrType': 'None',\n",
    "    'FireplaceQu': 'NA',\n",
    "    'LotFrontage': 'median_by_neighborhood',\n",
    "    'GarageType': 'NA',\n",
    "    'GarageFinish': 'NA',\n",
    "    'GarageQual': 'NA',\n",
    "    'GarageCond': 'NA',\n",
    "    'BsmtExposure': 'NA',\n",
    "    'BsmtFinType1': 'NA',\n",
    "    'BsmtFinType2': 'NA',\n",
    "    'BsmtQual': 'NA',\n",
    "    'BsmtCond': 'NA',\n",
    "    'Electrical': 'SBrkr' # Fill with moda value\n",
    "}\n",
    "\n",
    "def get_fill_value(col, dtype):\n",
    "    if col in default_fill:\n",
    "        return default_fill[col]\n",
    "    elif 'object' in str(dtype):\n",
    "        return 'None'\n",
    "    elif 'int' in str(dtype) or 'float' in str(dtype):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "missing_df['missing_value_fill_type'] = [\n",
    "    get_fill_value(col, missing_df.loc[col, 'dtype']) for col in missing_df.index\n",
    "]\n",
    "\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Estimate what values better to take to fill missing value cells for the numerical params: LotFrontage, GarageYrBlt, MasVnrArea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### LotFrontage\n",
    "There are 259 mv, and we should create plots to see if we can take median value to fill mv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['LotFrontage'].dropna(), kde=True, bins=40)\n",
    "plt.title('Distribution of LotFrontage')\n",
    "plt.xlabel('LotFrontage')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We will try to fill LotFrontage (Linear feet of street connected to property) mv by median of this feature grouped by Neighborhood (as it's value tend to be realistic). It's more stable for the outliers problem. There are other approaches, but we will keep going with median value for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "#     lambda x: x.fillna(x.median())\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### GarageYrBlt\n",
    "We can see that Garage properties are equally missed by several features:\n",
    "```\n",
    "GarageType               81   object                      NA\n",
    "GarageYrBlt              81  float64                       0\n",
    "GarageFinish             81   object                      NA\n",
    "GarageQual               81   object                      NA\n",
    "GarageCond               81   object                      NA\n",
    "```\n",
    "\n",
    "We can conlude that some Houses have no garage. In future we can create binary feature HasGarage and ommit some features if they will have no prediction power.\n",
    "\n",
    "Fill mv GarageYrBlt = 0 (Ganage not exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### MasVnrArea\n",
    "It is logical to fill in 0 - there is no area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['MasVnrArea'] = train['MasVnrArea'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Fill missing values using missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(train, missing_df):\n",
    "    for col, row in missing_df.iterrows():\n",
    "        \"\"\"\n",
    "        col → index of the row in missing_df (the column name in our train/test df)\n",
    "        row → the entire row as a Series with all its fields\n",
    "\n",
    "        col = \"LotFrontage\"\n",
    "        row = Series(\n",
    "            missing_count=259,\n",
    "            dtype=\"float64\",\n",
    "            missing_value_fill_type=\"median_by_neighborhood\"\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        fill_type = row['missing_value_fill_type']\n",
    "\n",
    "        # train[col + '_was_missing'] = train[col].isnull().astype(int)\n",
    "        \n",
    "        if fill_type == \"median_by_neighborhood\":\n",
    "            train[col] = train.groupby('Neighborhood')[col].transform(\n",
    "                lambda x: x.fillna(x.median()) # By default, pandas ignores NaNs when calculating .median()\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if fill_type in {0, \"NA\", \"None\", \"Mix\"}:\n",
    "            train[col] = train[col].fillna(fill_type)\n",
    "            continue\n",
    "\n",
    "        print(f\"[WARN] Unknown fill_type '{fill_type}' for column '{col}'\")\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = train.groupby('Neighborhood')['LotFrontage']\n",
    "# for name, group in grouped:\n",
    "#     non_missing = group[~group.isnull()]\n",
    "#     print(f\"Neighborhood: {name}\")\n",
    "#     print(non_missing)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fill_missing_values(train.copy(), missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Check if all missing values were filled. DataFrame should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame({'missing_count':missing_values})\n",
    "missing_df['dtype'] = train[missing_df.index].dtypes\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Histogramm SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train[\"SalePrice\"], kde=True)\n",
    "plt.title(\"SalePrice Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train[\"LotArea\"], kde=True)\n",
    "plt.title(\"LotArea Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Boxplot for exploring outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=train[\"GrLivArea\"])\n",
    "plt.title(\"GrLivArea Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Show 10 recorts with the largest GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nlargest(10, 'GrLivArea')[['GrLivArea', 'SalePrice']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "There are 2 points that are seems to be not logical:\n",
    "\n",
    "GrLivArea = 5642, SalePrice = 160000\n",
    "\n",
    "GrLivArea = 4676, SalePrice = 184750\n",
    "\n",
    "it's big areas for a very small price. This you can also see on a further scatter plot \"GrLivArea vs SalePrice\" - 2 points on the right down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=train[\"LotArea\"])\n",
    "plt.title(\"LotArea Boxplot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=train[\"TotalBsmtSF\"])\n",
    "plt.title(\"TotalBsmtSF Boxplot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Scatter-plot: GrLivArea → SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=train[\"GrLivArea\"], y=train[\"SalePrice\"])\n",
    "plt.title(\"GrLivArea vs SalePrice\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "corr = train.corr(numeric_only=True)\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = corr[\"SalePrice\"].abs().sort_values(ascending=False).head(10)\n",
    "top_corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "We have multicolleniarity for some features. Will left them as is. Will see how it affects different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Search for the important categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=\"OverallQual\", y=\"SalePrice\", data=train, estimator=\"mean\")\n",
    "plt.title(\"Average SalePrice by OverallQual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='OverallQual', y='SalePrice', data=train)\n",
    "plt.title(\"SalePrice vs OverallQual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_val, p_val = stats.f_oneway(\n",
    "    train[train['OverallQual']==1]['SalePrice'],\n",
    "    train[train['OverallQual']==2]['SalePrice'],\n",
    "    train[train['OverallQual']==3]['SalePrice'],\n",
    "    train[train['OverallQual']==4]['SalePrice'],\n",
    "    train[train['OverallQual']==5]['SalePrice'],\n",
    "    # train[train['OverallQual']==6]['SalePrice'],\n",
    "    # train[train['OverallQual']==7]['SalePrice'],\n",
    "    # train[train['OverallQual']==8]['SalePrice'],\n",
    "    # train[train['OverallQual']==9]['SalePrice'],\n",
    "    # train[train['OverallQual']==10]['SalePrice'],\n",
    "\n",
    "    # ...\n",
    ")\n",
    "print(f\"F={f_val}, p={p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = train[(train['GrLivArea'] > 4500) & (train['SalePrice'] < 300000)].index\n",
    "\n",
    "train = train.drop(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=train[\"GrLivArea\"], y=train[\"SalePrice\"])\n",
    "plt.title(\"GrLivArea vs SalePrice\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# 4. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Find categorical features and explore their values\n",
    "\n",
    "We need it to understand what type of encoding we should apply:\n",
    "- OneHot Encoding - for nominal categorical features where there is no order.\n",
    "- Ordinal Encoding - converts a categorical feature to numbers, preserving the order of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all categorical features represented by strings\n",
    "categorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    unique_vals = train[col].unique()\n",
    "    print(f\"\\n{col} ({len(unique_vals)} unique values):\")\n",
    "    print(unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### Find if there are categorical features that are represented by numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if train[col].nunique() < 20: \n",
    "        print(col, '- categorical?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "```\n",
    "MSSubClass - categorical ohe\n",
    "OverallQual - categorical ordinal\n",
    "OverallCond - categorical ordinal\n",
    "BsmtFullBath - count\n",
    "BsmtHalfBath - count\n",
    "FullBath - count\n",
    "HalfBath - count\n",
    "BedroomAbvGr - count\n",
    "KitchenAbvGr - count\n",
    "TotRmsAbvGrd - count\n",
    "Fireplaces - count\n",
    "GarageCars - count\n",
    "PoolArea - numerical\n",
    "MoSold - numerical cyclic\n",
    "YrSold - categorical ohe but we will create new feature based on this. no need for OHE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Transform numeric categorical features to string type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_numeric_features = ['MSSubClass']\n",
    "train[ohe_numeric_features] = train[ohe_numeric_features].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_numeric_features = ['OverallQual', 'OverallCond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclic_numeric_features = ['MoSold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### Ordinal encoding\n",
    "\n",
    "Provided data_description.txt to ChatGPT to figure out where we should apply ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = [\n",
    "    'ExterQual',      \n",
    "    'ExterCond',      \n",
    "    'BsmtQual',       \n",
    "    'BsmtCond',       \n",
    "    'KitchenQual',    \n",
    "    'GarageQual',     \n",
    "    'GarageCond',     \n",
    "    'FireplaceQu',    \n",
    "    'PoolQC',         \n",
    "    'Functional',     \n",
    "    'GarageFinish',   \n",
    "    'BsmtExposure',   \n",
    "    'BsmtFinType1',   \n",
    "    'BsmtFinType2',   \n",
    "    'HeatingQC'       \n",
    "] + ordinal_numeric_features\n",
    "\n",
    "# ExterQual, ExterCond, BsmtQual, BsmtCond, KitchenQual, FireplaceQu, GarageQual, GarageCond, PoolQC\n",
    "quality_map = {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "# BsmtExposure\n",
    "bsmt_exposure_map = {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "# GarageFinish\n",
    "garage_finish_map = {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "# BsmtFinType1, BsmtFinType2\n",
    "bsmt_fin_type_map = {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "# Functional\n",
    "functional_map = {'NA': 0, 'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8}\n",
    "\n",
    "ordinal_maps = {\n",
    "    'ExterQual': quality_map,\n",
    "    'ExterCond': quality_map,\n",
    "    'BsmtQual': quality_map,\n",
    "    'BsmtCond': quality_map,\n",
    "    'HeatingQC': quality_map,\n",
    "    'KitchenQual': quality_map,\n",
    "    'FireplaceQu': quality_map,\n",
    "    'GarageQual': quality_map,\n",
    "    'GarageCond': quality_map,\n",
    "    'PoolQC': quality_map,\n",
    "\n",
    "    'BsmtExposure': bsmt_exposure_map,\n",
    "    'GarageFinish': garage_finish_map,\n",
    "\n",
    "    'BsmtFinType1': bsmt_fin_type_map,\n",
    "    'BsmtFinType2': bsmt_fin_type_map,\n",
    "\n",
    "    'Functional': functional_map\n",
    "}\n",
    "\n",
    "for col, mapping in ordinal_maps.items():\n",
    "    train[col] = train[col].map(mapping).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### One Hot encoding\n",
    "\n",
    "Asked ChatGTP to find nominal features to apply simple OHE (without grouping). \n",
    "\n",
    "Note: Some of them can have numeric values. Previously we transformed such features to `str` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features = [\n",
    "    'MSZoning', \n",
    "    'Street', \n",
    "    'Alley', \n",
    "    'LotShape', \n",
    "    'LandContour', \n",
    "    'Utilities', \n",
    "    'LotConfig', \n",
    "    'LandSlope', \n",
    "    'BldgType', \n",
    "    'HouseStyle', \n",
    "    'RoofStyle', \n",
    "    'MasVnrType', \n",
    "    'Foundation',\n",
    "    'Heating', \n",
    "    'CentralAir', \n",
    "    'GarageType',\n",
    "    'PavedDrive', \n",
    "    'MiscFeature', \n",
    "    'Fence',\n",
    "    'SaleType', \n",
    "    'SaleCondition',\n",
    "    'Electrical'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Find features what need grouping because of their rareness. \n",
    "\n",
    "It can lead to overfitting, so we group rare values of categorical features to 'Other' group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_with_grouping = [\n",
    "    'Exterior1st',\n",
    "    'Exterior2nd',\n",
    "    'Neighborhood',\n",
    "    'Condition1',\n",
    "    'Condition2',\n",
    "    'RoofMatl'\n",
    "] + ohe_numeric_features\n",
    "\n",
    "def group_rare_categories(df, col, min_freq=20):\n",
    "    freqs = df[col].value_counts()\n",
    "    rare = freqs[freqs < min_freq].index\n",
    "    df[col] = df[col].replace(rare, 'Other')\n",
    "    return df\n",
    "\n",
    "for col in ohe_with_grouping:\n",
    "    train = group_rare_categories(train, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all feature for OHE\n",
    "ohe_features += ohe_with_grouping\n",
    "\n",
    "# Apply OHE. Drop 1st column to avoid milticolliniarity. We need it to apply linear regression\n",
    "train = pd.get_dummies(\n",
    "    train,\n",
    "    columns=ohe_features,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# To apply tree model - need to apply drop_first=False\n",
    "# train = pd.get_dummies(\n",
    "#     train,\n",
    "#     columns=ohe_features,\n",
    "#     drop_first=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "#### Check if all categorical features were encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)      # show all rows\n",
    "pd.set_option(\"display.max_columns\", None)   # show all columns\n",
    "pd.set_option(\"display.max_colwidth\", None)  # show full column names\n",
    "pd.set_option(\"display.width\", None)         # don't wrap lines\n",
    "train.dtypes\n",
    "# pd.reset_option(\"display.max_rows\")\n",
    "# pd.reset_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select_dtypes(include='object').shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = train.columns[\n",
    "    (train.dtypes == 'object') | \n",
    "    (train.dtypes == 'category')\n",
    "]\n",
    "\n",
    "bad_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### Numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "#### Cyclic data transformation - sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['MoSold_sin'] = np.sin(2 * np.pi * train['MoSold'] / 12)\n",
    "train['MoSold_cos'] = np.cos(2 * np.pi * train['MoSold'] / 12)\n",
    "\n",
    "cyclic_numeric_features = ['MoSold_cos', 'MoSold_sin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "#### Create new numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total square\n",
    "train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "# total bath amount\n",
    "train['TotalBath'] = train['FullBath'] + 0.5 * train['HalfBath'] + train['BsmtFullBath'] + 0.5 * train['BsmtHalfBath']\n",
    "# age of house on the moment it was sold\n",
    "train['AgeAtSale'] = train['YrSold'] - train['YearBuilt']\n",
    "# where house remodeled (repaired) or not\n",
    "train['Remodeled'] = (train['YearRemodAdd'] != train['YearBuilt']).astype(int)\n",
    "# garage age\n",
    "train['GarageAge'] = train['YrSold'] - train['GarageYrBlt']\n",
    "# if there no harage - fill with 0\n",
    "train['GarageAge'] = train['GarageAge'].fillna(0)\n",
    "# is the house is new or not\n",
    "train['IsNew'] = (train['YrSold'] == train['YearBuilt']).astype(int)\n",
    "\n",
    "new_features = ['TotalSF', 'TotalBath', 'AgeAtSale', 'Remodeled', 'GarageAge', 'IsNew']\n",
    "print(train[new_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### Remove redundant numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['YrSold', 'MoSold']\n",
    "train.drop(columns=features_to_drop, inplace=True)\n",
    "numeric_cols = [x for x in numeric_cols if x not in features_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Log-transform numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Log-transform target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log1p(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train['SalePrice'], kde=True)\n",
    "plt.title('SalePrice (Log) Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "#### Log transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude target\n",
    "numeric_cols.remove('SalePrice')\n",
    "\n",
    "# Exclude categorical features that are represented in numeric format\n",
    "all_categorical_features = ordinal_features + ohe_features\n",
    "\n",
    "# Exclude cyclic numerical features\n",
    "transformed_features = all_categorical_features + cyclic_numeric_features\n",
    "\n",
    "numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in transformed_features\n",
    "]\n",
    "\n",
    "# Exclude binary features\n",
    "numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if train[col].nunique() > 2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Summary for all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = pd.DataFrame({\n",
    "#     'nunique': train[numeric_cols].nunique(),\n",
    "#     'min': train[numeric_cols].min(),\n",
    "#     'max': train[numeric_cols].max(),\n",
    "#     'skew': train[numeric_cols].apply(lambda x: skew(x.dropna()))\n",
    "# }).sort_values('skew', ascending=False)\n",
    "\n",
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### Find and exclude featues that semantically not needed to be log-transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "We need to log-transform only continuous features like area, volume, price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_exclude = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    unique_vals = train[col].nunique()\n",
    "    max_val = train[col].max()\n",
    "\n",
    "    # categorical / discrete\n",
    "    if unique_vals < 20:\n",
    "        semantic_exclude.append(col)\n",
    "\n",
    "    # counters\n",
    "    if max_val <= 10 and unique_vals <= 10:\n",
    "        semantic_exclude.append(col)\n",
    "\n",
    "loggable_numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in semantic_exclude\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "#### Plots of numeric feature disctibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in loggable_numeric_cols:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "#### Skew of disctributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "skewed_features = train[loggable_numeric_cols].apply(lambda x : skew(x.dropna()))\n",
    "skewed_features = skewed_features[abs(skewed_features) > 0.75].index.tolist()\n",
    "\n",
    "skewed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in skewed_features:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "#### Show correlation with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = []\n",
    "for col in loggable_numeric_cols:\n",
    "    corr_original = train[col].corr(train['SalePrice'])\n",
    "    corr_log = np.log1p(train[col]).corr(train['SalePrice'])\n",
    "    if abs(corr_log) > abs(corr_original):\n",
    "        correlated_features.append(col)\n",
    "\n",
    "correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "#### Find unique values for skewed and correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features_unique = [\n",
    "    col for col in skewed_features \n",
    "    if col not in correlated_features\n",
    "]\n",
    "skewed_features_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_candidates_unique = [\n",
    "    col for col in correlated_features \n",
    "    if col not in skewed_features\n",
    "]\n",
    "log_candidates_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "#### Unite skewed and correlated features\n",
    "\n",
    "They may have different feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_candidates_features = set(skewed_features) | set(correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "#### Compare log candidates and skewed + correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggable_features_but_not_candidates = set(loggable_numeric_cols) - set(log_candidates_features)\n",
    "loggable_features_but_not_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in loggable_features_but_not_candidates:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "#### Apply log-transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after operations with set log_candidates_features have type set[Hashable | Any]\n",
    "# need to cast it back to list[str]\n",
    "log_candidates_features: list[str] = list(log_candidates_features)\n",
    "for col in log_candidates_features:\n",
    "    train[col] = np.log1p(train[col])\n",
    "\n",
    "# for col in log_candidates_final:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "#### Find sparse features and create binary columns for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = train[log_candidates_features].apply(lambda x: (x == 0).mean())\n",
    "sparse_features = sparsity[sparsity > 0.5].index.tolist()  # >50% zeros\n",
    "\n",
    "for col in sparse_features:\n",
    "    train[f'Has_{col}'] = (train[col] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "# 5. Preapare for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "### Create target and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['SalePrice']\n",
    "x_train = train.drop(columns=['SalePrice', 'Id'])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### Transforming pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler_candidates = log_candidates_features + cyclic_numeric_features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), standard_scaler_candidates)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "#### Train / validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "For initial test it's a good solution. \n",
    "To check: \n",
    "- if there no leakage\n",
    "- if pipeline works correct\n",
    "- are the metrics adequate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "#### KFold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "Good solution for:\n",
    "- precise estimation\n",
    "- fair comparison\n",
    "- hyperparams tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "#### Train on a whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "Fit a whole dataset. Then predict on a test. Need to be done in the end - before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "# 6. Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "## Evaluation strategy\n",
    "\n",
    "- Primary metric: RMSE\n",
    "- Target transformation: log1p(SalePrice)\n",
    "- Evaluation method: 5-fold cross-validation\n",
    "- Model selection based on CV RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return root_mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "## Baseline №1 — DummyRegressor\n",
    "\n",
    "Main purpose:\n",
    "- predicts median: y_pred_dummy = median(y_train)\n",
    "- adequacy control\n",
    "- sanity check\n",
    "\n",
    "Expected values:\n",
    "- RMSE should be bad (~0.35-0.40)\n",
    "- if RMSE is good - we have leakage\n",
    "\n",
    "The baseline is a starting point, a simple model that demonstrates the minimum possible performance we expect from any \"reasonable\" model.\n",
    "- It doesn't use features, so it honestly shows \"how easy it is to guess without knowing anything about the data.\"\n",
    "- All subsequent models should show improvement over the baseline.\n",
    "\n",
    "1. Simple logic:\n",
    "    - DummyRegressor simply calculates the mean/median of the target and always predicts the same value.\n",
    "    - It doesn't use X → which means its real improvement comes only from the fact that the model \"learns from features.\"\n",
    "2. Strong robustness to outliers (if the median)\n",
    "    - In our data, SalePrice is heavily skewed → the median provides a fair baseline, uncorrupted by rare expensive houses.\n",
    "3. Leakage Control\n",
    "   - If your complex model performs worse than Dummy → this is a sign that:\n",
    "        - the pipeline is malfunctioning,\n",
    "        - there are data leaks,\n",
    "        - the features are not aligned with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dummy = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "dummy_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', dummy)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_dummy = cross_val_score(\n",
    "    dummy_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "dummy_rmse = -scores_dummy.mean()\n",
    "print(dummy_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "## Baseline №2 — LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "We build this model because:\n",
    "- it's simple - we can choose it as a baseline to check other complecated models\n",
    "    - LinearRegression → baseline with \"minimal processing\"\n",
    "    - Ridge / Lasso → regularization check\n",
    "    - RandomForest / Boosting → complex models\n",
    "\n",
    "\n",
    "- check if we have a liner signal\n",
    "- figure out how features are adequate\n",
    "- expected RMSE ≈ 0.20–0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "\n",
    "linear_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', linear)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_linear = cross_val_score(\n",
    "    linear_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "linear_rmse = -scores_linear.mean()\n",
    "print(linear_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "We got a very good score for the 2nd baseline - LinearRegression.\n",
    "\n",
    "The very good RMSE of LinearRegression indicates that there is a strong linear signal in the data.\n",
    "However, due to the presence of highly correlated features, the coefficients of the linear model may be unstable and hard to interpret.\n",
    "\n",
    "Multicollinearity does not necessarily worsen predictive performance, but it leads to unstable and non-interpretable coefficients in linear models. Therefore, highly correlated features are removed to obtain a stable and interpretable baseline.\n",
    "\n",
    "The goal was not to improve the score, but to obtain a cleaner and more interpretable baseline.\n",
    "This allows a fair comparison with more complex models and helps separate the effect of feature engineering from the effect of model complexity.\n",
    "\n",
    "We remove highly correlated features not to handicap linear models, but to obtain a stable and interpretable baseline that allows us to isolate the effect of model complexity.\n",
    "\n",
    "If we remove the correlations and look at the RMSE of the linear model:\n",
    "- It's easier to see how much the complex model actually improves the results.\n",
    "- It's easy to argue that \"Boosting gives +0.03 RMSE because the linear model gave 0.18, and the baseline was 0.22.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "Why remove highly correlated features for LinearRegression when creating a baseline:\n",
    "\n",
    "1. Coefficient Stability\n",
    "    - Highly correlated features → linear model coefficients are unstable\n",
    "    - With repeated splits (KFold), the coefficients \"jump\"\n",
    "    - Removing correlations makes the baseline predictable and stable\n",
    "2. Feature Interpretation\n",
    "    - Coefficients become meaningful: it's easy to understand which feature really influences the target\n",
    "    - Without correlated features, it's impossible to provide a correct explanation, for example, why increasing the area of ​​a house increases the price\n",
    "3. Fair Comparison with More Complex Models\n",
    "    - Removing redundant information so that the baseline doesn't receive an \"unnecessary bonus\" from duplicate features\n",
    "    - Allows you to accurately assess how much improvement a complex model (Boosting, RF, etc.) provides, and how much is simply due to good feature engineering\n",
    "4. Isolating the Effect of the Model\n",
    "    - Separating the Contribution of Data (Feature Engineering) and the Algorithm\n",
    "    - The RMSE of the clean baseline shows how much a simple linear model can provide\n",
    "    - Any improvement in a complex model now fairly reflects the advantage of the algorithm, not the features correlated data\n",
    "\n",
    "Conclusion:\n",
    "Highly correlated features are removed in the linear baseline not to worsen RMSE, but to obtain stable, interpretable coefficients and a fair comparison with more complex models. This allows us to isolate the contribution of model complexity from the effect of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "We should find the most correlated features to exclude them from model.\n",
    "\n",
    "There are 2 approaches how to do it:\n",
    "- feature–feature correlation\n",
    "- VIF (Variance Inflation Factor)\n",
    "\n",
    "Let's find the most correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = x_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = x_train.select_dtypes(include=['object']).columns\n",
    "boolean_features = x_train.select_dtypes(include=['boolean']).columns\n",
    "\n",
    "print(\"numerical_features:\", numerical_features)\n",
    "print(\"categorical_features:\", categorical_features)\n",
    "print(\"boolean_features:\", boolean_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of numerical features with SalePrice\n",
    "correlations = x_train[numerical_features].copy()\n",
    "correlations['SalePrice'] = y_train\n",
    "corr_matrix = correlations.corr()\n",
    "\n",
    "# Sort by absolute correlation with SalePrice\n",
    "top_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False)\n",
    "print(top_corr.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_features = top_corr.index[1:11]  # exclude SalePrice\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    sns.scatterplot(x=x_train[feature], y=y_train)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
