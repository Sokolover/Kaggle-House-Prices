{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Recommended tools:\n",
    "- Python 3.11+\n",
    "- VSCode\n",
    "- Data Wrangler - to explare data in the code output\n",
    "- nbstripout - to automatically omit jupiter notebook output in git commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 1. Import and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Count missing values and attach their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame({'missing_count':missing_values})\n",
    "missing_df['dtype'] = train[missing_df.index].dtypes\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Visualise missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,10))\n",
    "sns.heatmap(train.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title(\"Missing values in train.csv\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 2. Process missing values (mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "I looked into data_description.txt and found values that we can substitute for the NaN values for the categorical data.\n",
    "Then added manually missing_value_fill_type as the last column in the dataframe:\n",
    "```\n",
    "              missing_count    dtype missing_value_fill_type\n",
    "PoolQC                 1453   object    NA\n",
    "MiscFeature            1406   object    NA\n",
    "Alley                  1369   object    NA\n",
    "Fence                  1179   object    NA\n",
    "MasVnrType              872   object    None\n",
    "FireplaceQu             690   object    NA\n",
    "LotFrontage             259  float64\n",
    "GarageType               81   object    NA\n",
    "GarageYrBlt              81  float64\n",
    "GarageFinish             81   object    NA\n",
    "GarageQual               81   object    NA\n",
    "GarageCond               81   object    NA\n",
    "BsmtExposure             38   object    NA\n",
    "BsmtFinType2             38   object    NA\n",
    "BsmtQual                 37   object    NA\n",
    "BsmtCond                 37   object    NA\n",
    "BsmtFinType1             37   object    NA\n",
    "MasVnrArea                8  float64\n",
    "Electrical                1   object    # Not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Create df that will represent data described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with default values ​​for columns with gaps.\n",
    "# There can be special default values like 'median_by_neighborhood' - they will have specific processing rules in process missing values function.\n",
    "# Here we only specify special cases; the rest will be handled automatically (like 0 for int/float values).\n",
    "default_fill = {\n",
    "    'PoolQC': 'NA',\n",
    "    'MiscFeature': 'NA',\n",
    "    'Alley': 'NA',\n",
    "    'Fence': 'NA',\n",
    "    'MasVnrType': 'None',\n",
    "    'FireplaceQu': 'NA',\n",
    "    'LotFrontage': 'median_by_neighborhood',\n",
    "    'GarageType': 'NA',\n",
    "    'GarageFinish': 'NA',\n",
    "    'GarageQual': 'NA',\n",
    "    'GarageCond': 'NA',\n",
    "    'BsmtExposure': 'NA',\n",
    "    'BsmtFinType1': 'NA',\n",
    "    'BsmtFinType2': 'NA',\n",
    "    'BsmtQual': 'NA',\n",
    "    'BsmtCond': 'NA',\n",
    "    'Electrical': 'SBrkr' # Fill with moda value\n",
    "}\n",
    "\n",
    "def get_fill_value(col, dtype):\n",
    "    if col in default_fill:\n",
    "        return default_fill[col]\n",
    "    elif 'object' in str(dtype):\n",
    "        return 'None'\n",
    "    elif 'int' in str(dtype) or 'float' in str(dtype):\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "missing_df['missing_value_fill_type'] = [\n",
    "    get_fill_value(col, missing_df.loc[col, 'dtype']) for col in missing_df.index\n",
    "]\n",
    "\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Estimate what values better to take to fill missing value cells for the numerical params: LotFrontage, GarageYrBlt, MasVnrArea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### LotFrontage\n",
    "There are 259 mv, and we should create plots to see if we can take median value to fill mv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['LotFrontage'].dropna(), kde=True, bins=40)\n",
    "plt.title('Distribution of LotFrontage')\n",
    "plt.xlabel('LotFrontage')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We will try to fill LotFrontage (Linear feet of street connected to property) mv by median of this feature grouped by Neighborhood (as it's value tend to be realistic). It's more stable for the outliers problem. There are other approaches, but we will keep going with median value for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "#     lambda x: x.fillna(x.median())\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### GarageYrBlt\n",
    "We can see that Garage properties are equally missed by several features:\n",
    "```\n",
    "GarageType               81   object                      NA\n",
    "GarageYrBlt              81  float64                       0\n",
    "GarageFinish             81   object                      NA\n",
    "GarageQual               81   object                      NA\n",
    "GarageCond               81   object                      NA\n",
    "```\n",
    "\n",
    "We can conlude that some Houses have no garage. In future we can create binary feature HasGarage and ommit some features if they will have no prediction power.\n",
    "\n",
    "Fill mv GarageYrBlt = 0 (Ganage not exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### MasVnrArea\n",
    "It is logical to fill in 0 - there is no area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['MasVnrArea'] = train['MasVnrArea'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Fill missing values using missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(train, missing_df):\n",
    "    for col, row in missing_df.iterrows():\n",
    "        \"\"\"\n",
    "        col → index of the row in missing_df (the column name in our train/test df)\n",
    "        row → the entire row as a Series with all its fields\n",
    "\n",
    "        col = \"LotFrontage\"\n",
    "        row = Series(\n",
    "            missing_count=259,\n",
    "            dtype=\"float64\",\n",
    "            missing_value_fill_type=\"median_by_neighborhood\"\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        fill_type = row['missing_value_fill_type']\n",
    "\n",
    "        # train[col + '_was_missing'] = train[col].isnull().astype(int)\n",
    "        \n",
    "        if fill_type == \"median_by_neighborhood\":\n",
    "            train[col] = train.groupby('Neighborhood')[col].transform(\n",
    "                lambda x: x.fillna(x.median()) # By default, pandas ignores NaNs when calculating .median()\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if fill_type in {0, \"NA\", \"None\", \"Mix\"}:\n",
    "            train[col] = train[col].fillna(fill_type)\n",
    "            continue\n",
    "\n",
    "        print(f\"[WARN] Unknown fill_type '{fill_type}' for column '{col}'\")\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = train.groupby('Neighborhood')['LotFrontage']\n",
    "# for name, group in grouped:\n",
    "#     non_missing = group[~group.isnull()]\n",
    "#     print(f\"Neighborhood: {name}\")\n",
    "#     print(non_missing)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fill_missing_values(train.copy(), missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Check if all missing values were filled. DataFrame should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame({'missing_count':missing_values})\n",
    "missing_df['dtype'] = train[missing_df.index].dtypes\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Histogramm SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train[\"SalePrice\"], kde=True)\n",
    "plt.title(\"SalePrice Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train[\"LotArea\"], kde=True)\n",
    "plt.title(\"LotArea Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Boxplot for exploring outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=train[\"GrLivArea\"])\n",
    "plt.title(\"GrLivArea Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Show 10 recorts with the largest GrLivArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nlargest(10, 'GrLivArea')[['GrLivArea', 'SalePrice']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "There are 2 points that are seems to be not logical:\n",
    "\n",
    "GrLivArea = 5642, SalePrice = 160000\n",
    "\n",
    "GrLivArea = 4676, SalePrice = 184750\n",
    "\n",
    "it's big areas for a very small price. This you can also see on a further scatter plot \"GrLivArea vs SalePrice\" - 2 points on the right down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=train[\"LotArea\"])\n",
    "plt.title(\"LotArea Boxplot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=train[\"TotalBsmtSF\"])\n",
    "plt.title(\"TotalBsmtSF Boxplot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Scatter-plot: GrLivArea → SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=train[\"GrLivArea\"], y=train[\"SalePrice\"])\n",
    "plt.title(\"GrLivArea vs SalePrice\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "corr = train.corr(numeric_only=True)\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = corr[\"SalePrice\"].abs().sort_values(ascending=False).head(10)\n",
    "top_corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We have multicolleniarity for some features. Will left them as is. Will see how it affects different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Search for the important categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=\"OverallQual\", y=\"SalePrice\", data=train, estimator=\"mean\")\n",
    "plt.title(\"Average SalePrice by OverallQual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='OverallQual', y='SalePrice', data=train)\n",
    "plt.title(\"SalePrice vs OverallQual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_val, p_val = stats.f_oneway(\n",
    "    train[train['OverallQual']==1]['SalePrice'],\n",
    "    train[train['OverallQual']==2]['SalePrice'],\n",
    "    train[train['OverallQual']==3]['SalePrice'],\n",
    "    train[train['OverallQual']==4]['SalePrice'],\n",
    "    train[train['OverallQual']==5]['SalePrice'],\n",
    "    # train[train['OverallQual']==6]['SalePrice'],\n",
    "    # train[train['OverallQual']==7]['SalePrice'],\n",
    "    # train[train['OverallQual']==8]['SalePrice'],\n",
    "    # train[train['OverallQual']==9]['SalePrice'],\n",
    "    # train[train['OverallQual']==10]['SalePrice'],\n",
    "\n",
    "    # ...\n",
    ")\n",
    "print(f\"F={f_val}, p={p_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = train[(train['GrLivArea'] > 4500) & (train['SalePrice'] < 300000)].index\n",
    "\n",
    "train = train.drop(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=train[\"GrLivArea\"], y=train[\"SalePrice\"])\n",
    "plt.title(\"GrLivArea vs SalePrice\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "# 4. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### Find categorical features and explore their values\n",
    "\n",
    "We need it to understand what type of encoding we should apply:\n",
    "- OneHot Encoding - for nominal categorical features where there is no order.\n",
    "- Ordinal Encoding - converts a categorical feature to numbers, preserving the order of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all categorical features represented by strings\n",
    "categorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    unique_vals = train[col].unique()\n",
    "    print(f\"\\n{col} ({len(unique_vals)} unique values):\")\n",
    "    print(unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Find if there are categorical features that are represented by numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if train[col].nunique() < 20: \n",
    "        print(col, '- categorical?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "```\n",
    "MSSubClass - categorical ohe\n",
    "OverallQual - categorical ordinal\n",
    "OverallCond - categorical ordinal\n",
    "BsmtFullBath - count\n",
    "BsmtHalfBath - count\n",
    "FullBath - count\n",
    "HalfBath - count\n",
    "BedroomAbvGr - count\n",
    "KitchenAbvGr - count\n",
    "TotRmsAbvGrd - count\n",
    "Fireplaces - count\n",
    "GarageCars - count\n",
    "PoolArea - numerical\n",
    "MoSold - numerical cyclic\n",
    "YrSold - categorical ohe but we will create new feature based on this. no need for OHE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "#### Transform numeric categorical features to string type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_numeric_features = ['MSSubClass']\n",
    "train[ohe_numeric_features] = train[ohe_numeric_features].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_numeric_features = ['OverallQual', 'OverallCond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclic_numeric_features = ['MoSold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Ordinal encoding\n",
    "\n",
    "Provided data_description.txt to ChatGPT to figure out where we should apply ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = [\n",
    "    'ExterQual',      \n",
    "    'ExterCond',      \n",
    "    'BsmtQual',       \n",
    "    'BsmtCond',       \n",
    "    'KitchenQual',    \n",
    "    'GarageQual',     \n",
    "    'GarageCond',     \n",
    "    'FireplaceQu',    \n",
    "    'PoolQC',         \n",
    "    'Functional',     \n",
    "    'GarageFinish',   \n",
    "    'BsmtExposure',   \n",
    "    'BsmtFinType1',   \n",
    "    'BsmtFinType2',   \n",
    "    'HeatingQC'       \n",
    "] + ordinal_numeric_features\n",
    "\n",
    "# ExterQual, ExterCond, BsmtQual, BsmtCond, KitchenQual, FireplaceQu, GarageQual, GarageCond, PoolQC\n",
    "quality_map = {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "# BsmtExposure\n",
    "bsmt_exposure_map = {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "# GarageFinish\n",
    "garage_finish_map = {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "# BsmtFinType1, BsmtFinType2\n",
    "bsmt_fin_type_map = {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "# Functional\n",
    "functional_map = {'NA': 0, 'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8}\n",
    "\n",
    "ordinal_maps = {\n",
    "    'ExterQual': quality_map,\n",
    "    'ExterCond': quality_map,\n",
    "    'BsmtQual': quality_map,\n",
    "    'BsmtCond': quality_map,\n",
    "    'HeatingQC': quality_map,\n",
    "    'KitchenQual': quality_map,\n",
    "    'FireplaceQu': quality_map,\n",
    "    'GarageQual': quality_map,\n",
    "    'GarageCond': quality_map,\n",
    "    'PoolQC': quality_map,\n",
    "\n",
    "    'BsmtExposure': bsmt_exposure_map,\n",
    "    'GarageFinish': garage_finish_map,\n",
    "\n",
    "    'BsmtFinType1': bsmt_fin_type_map,\n",
    "    'BsmtFinType2': bsmt_fin_type_map,\n",
    "\n",
    "    'Functional': functional_map\n",
    "}\n",
    "\n",
    "for col, mapping in ordinal_maps.items():\n",
    "    train[col] = train[col].map(mapping).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "#### One Hot encoding\n",
    "\n",
    "Asked ChatGTP to find nominal features to apply simple OHE (without grouping). \n",
    "\n",
    "Note: Some of them can have numeric values. Previously we transformed such features to `str` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features = [\n",
    "    'MSZoning', \n",
    "    'Street', \n",
    "    'Alley', \n",
    "    'LotShape', \n",
    "    'LandContour', \n",
    "    'Utilities', \n",
    "    'LotConfig', \n",
    "    'LandSlope', \n",
    "    'BldgType', \n",
    "    'HouseStyle', \n",
    "    'RoofStyle', \n",
    "    'MasVnrType', \n",
    "    'Foundation',\n",
    "    'Heating', \n",
    "    'CentralAir', \n",
    "    'GarageType',\n",
    "    'PavedDrive', \n",
    "    'MiscFeature', \n",
    "    'Fence',\n",
    "    'SaleType', \n",
    "    'SaleCondition',\n",
    "    'Electrical'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Find features what need grouping because of their rareness. \n",
    "\n",
    "It can lead to overfitting, so we group rare values of categorical features to 'Other' group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_with_grouping = [\n",
    "    'Exterior1st',\n",
    "    'Exterior2nd',\n",
    "    'Neighborhood',\n",
    "    'Condition1',\n",
    "    'Condition2',\n",
    "    'RoofMatl'\n",
    "] + ohe_numeric_features\n",
    "\n",
    "def group_rare_categories(df, col, min_freq=20):\n",
    "    freqs = df[col].value_counts()\n",
    "    rare = freqs[freqs < min_freq].index\n",
    "    df[col] = df[col].replace(rare, 'Other')\n",
    "    return df\n",
    "\n",
    "for col in ohe_with_grouping:\n",
    "    train = group_rare_categories(train, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all feature for OHE\n",
    "ohe_features += ohe_with_grouping\n",
    "\n",
    "# Apply OHE. Drop 1st column to avoid milticolliniarity. We need it to apply linear regression\n",
    "train = pd.get_dummies(\n",
    "    train,\n",
    "    columns=ohe_features,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# To apply tree model - need to apply drop_first=False\n",
    "# train = pd.get_dummies(\n",
    "#     train,\n",
    "#     columns=ohe_features,\n",
    "#     drop_first=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "#### Check if all categorical features were encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)      # show all rows\n",
    "pd.set_option(\"display.max_columns\", None)   # show all columns\n",
    "pd.set_option(\"display.max_colwidth\", None)  # show full column names\n",
    "pd.set_option(\"display.width\", None)         # don't wrap lines\n",
    "train.dtypes\n",
    "# pd.reset_option(\"display.max_rows\")\n",
    "# pd.reset_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select_dtypes(include='object').shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = train.columns[\n",
    "    (train.dtypes == 'object') | \n",
    "    (train.dtypes == 'category')\n",
    "]\n",
    "\n",
    "bad_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### Numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "#### Cyclic data transformation - sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['MoSold_sin'] = np.sin(2 * np.pi * train['MoSold'] / 12)\n",
    "train['MoSold_cos'] = np.cos(2 * np.pi * train['MoSold'] / 12)\n",
    "\n",
    "cyclic_numeric_features = ['MoSold_cos', 'MoSold_sin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Create new numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total square\n",
    "train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "# total bath amount\n",
    "train['TotalBath'] = train['FullBath'] + 0.5 * train['HalfBath'] + train['BsmtFullBath'] + 0.5 * train['BsmtHalfBath']\n",
    "# total porch square\n",
    "train['TotalPorchSF'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch']\n",
    "# age of house on the moment it was sold\n",
    "train['AgeAtSale'] = train['YrSold'] - train['YearBuilt']\n",
    "# where house remodeled (repaired) or not\n",
    "train['Remodeled'] = (train['YearRemodAdd'] != train['YearBuilt']).astype(int)\n",
    "# garage age\n",
    "train['GarageAge'] = train['YrSold'] - train['GarageYrBlt']\n",
    "# if there no harage - fill with 0\n",
    "train['GarageAge'] = train['GarageAge'].fillna(0)\n",
    "# is the house is new or not\n",
    "train['IsNew'] = (train['YrSold'] == train['YearBuilt']).astype(int)\n",
    "\n",
    "new_features = ['TotalSF', 'TotalBath', 'AgeAtSale', 'Remodeled', 'GarageAge', 'IsNew', 'TotalPorchSF']\n",
    "print(train[new_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "#### Remove redundant numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['YrSold', 'MoSold']\n",
    "train.drop(columns=features_to_drop, inplace=True)\n",
    "numeric_cols = [x for x in numeric_cols if x not in features_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### Log-transform numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "#### Log-transform target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log1p(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train['SalePrice'], kde=True)\n",
    "plt.title('SalePrice (Log) Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Log transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude target\n",
    "numeric_cols.remove('SalePrice')\n",
    "\n",
    "# Exclude categorical features that are represented in numeric format\n",
    "all_categorical_features = ordinal_features + ohe_features\n",
    "\n",
    "# Exclude cyclic numerical features\n",
    "transformed_features = all_categorical_features + cyclic_numeric_features\n",
    "\n",
    "numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in transformed_features\n",
    "]\n",
    "\n",
    "# Exclude binary features\n",
    "numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if train[col].nunique() > 2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "#### Summary for all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = pd.DataFrame({\n",
    "#     'nunique': train[numeric_cols].nunique(),\n",
    "#     'min': train[numeric_cols].min(),\n",
    "#     'max': train[numeric_cols].max(),\n",
    "#     'skew': train[numeric_cols].apply(lambda x: skew(x.dropna()))\n",
    "# }).sort_values('skew', ascending=False)\n",
    "\n",
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "#### Find and exclude featues that semantically not needed to be log-transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "We need to log-transform only continuous features like area, volume, price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_exclude = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    unique_vals = train[col].nunique()\n",
    "    max_val = train[col].max()\n",
    "\n",
    "    # categorical / discrete\n",
    "    if unique_vals < 20:\n",
    "        semantic_exclude.append(col)\n",
    "\n",
    "    # counters\n",
    "    if max_val <= 10 and unique_vals <= 10:\n",
    "        semantic_exclude.append(col)\n",
    "\n",
    "loggable_numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in semantic_exclude\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "#### Plots of numeric feature disctibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in loggable_numeric_cols:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "#### Skew of disctributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "skewed_features = train[loggable_numeric_cols].apply(lambda x : skew(x.dropna()))\n",
    "skewed_features = skewed_features[abs(skewed_features) > 0.75].index.tolist()\n",
    "\n",
    "skewed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in skewed_features:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "#### Show correlation with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = []\n",
    "for col in loggable_numeric_cols:\n",
    "    corr_original = train[col].corr(train['SalePrice'])\n",
    "    corr_log = np.log1p(train[col]).corr(train['SalePrice'])\n",
    "    if abs(corr_log) > abs(corr_original):\n",
    "        correlated_features.append(col)\n",
    "\n",
    "correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "#### Find unique values for skewed and correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features_unique = [\n",
    "    col for col in skewed_features \n",
    "    if col not in correlated_features\n",
    "]\n",
    "skewed_features_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_candidates_unique = [\n",
    "    col for col in correlated_features \n",
    "    if col not in skewed_features\n",
    "]\n",
    "log_candidates_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "#### Unite skewed and correlated features\n",
    "\n",
    "They may have different feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_candidates_features = set(skewed_features) | set(correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "#### Compare log candidates and skewed + correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggable_features_but_not_candidates = set(loggable_numeric_cols) - set(log_candidates_features)\n",
    "loggable_features_but_not_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in loggable_features_but_not_candidates:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "#### Apply log-transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after operations with set log_candidates_features have type set[Hashable | Any]\n",
    "# need to cast it back to list[str]\n",
    "log_candidates_features: list[str] = list(log_candidates_features)\n",
    "for col in log_candidates_features:\n",
    "    train[col] = np.log1p(train[col])\n",
    "\n",
    "# for col in log_candidates_final:\n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     sns.histplot(train[col], bins=50, kde=True)\n",
    "#     plt.title(col)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "#### Find sparse features and create binary columns for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = train[log_candidates_features].apply(lambda x: (x == 0).mean())\n",
    "sparse_features = sparsity[sparsity > 0.5].index.tolist()  # >50% zeros\n",
    "\n",
    "for col in sparse_features:\n",
    "    train[f'Has_{col}'] = (train[col] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "# 5. Preapare for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### Create target and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['SalePrice']\n",
    "x_train = train.drop(columns=['SalePrice', 'Id'])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "### Transforming pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler_candidates = log_candidates_features + cyclic_numeric_features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), standard_scaler_candidates)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "#### Train / validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "For initial test it's a good solution. \n",
    "To check: \n",
    "- if there no leakage\n",
    "- if pipeline works correct\n",
    "- are the metrics adequate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "#### KFold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "Good solution for:\n",
    "- precise estimation\n",
    "- fair comparison\n",
    "- hyperparams tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "#### Train on a whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "Fit a whole dataset. Then predict on a test. Need to be done in the end - before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "# 6. Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "## Evaluation strategy\n",
    "\n",
    "- Primary metric: RMSE\n",
    "- Target transformation: log1p(SalePrice)\n",
    "- Evaluation method: 5-fold cross-validation\n",
    "- Model selection based on CV RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return root_mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## Baseline №1 — DummyRegressor\n",
    "\n",
    "Main purpose:\n",
    "- predicts median: y_pred_dummy = median(y_train)\n",
    "- adequacy control\n",
    "- sanity check\n",
    "\n",
    "Expected values:\n",
    "- RMSE should be bad (~0.35-0.40)\n",
    "- if RMSE is good - we have leakage\n",
    "\n",
    "The baseline is a starting point, a simple model that demonstrates the minimum possible performance we expect from any \"reasonable\" model.\n",
    "- It doesn't use features, so it honestly shows \"how easy it is to guess without knowing anything about the data.\"\n",
    "- All subsequent models should show improvement over the baseline.\n",
    "\n",
    "1. Simple logic:\n",
    "    - DummyRegressor simply calculates the mean/median of the target and always predicts the same value.\n",
    "    - It doesn't use X → which means its real improvement comes only from the fact that the model \"learns from features.\"\n",
    "2. Strong robustness to outliers (if the median)\n",
    "    - In our data, SalePrice is heavily skewed → the median provides a fair baseline, uncorrupted by rare expensive houses.\n",
    "3. Leakage Control\n",
    "   - If your complex model performs worse than Dummy → this is a sign that:\n",
    "        - the pipeline is malfunctioning,\n",
    "        - there are data leaks,\n",
    "        - the features are not aligned with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dummy = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "dummy_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', dummy)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_dummy = cross_val_score(\n",
    "    dummy_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "dummy_rmse = -scores_dummy.mean()\n",
    "print(dummy_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "## Baseline №2 — LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### Default model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "We build this model because:\n",
    "- it's simple - we can choose it as a baseline to check other complecated models\n",
    "    - LinearRegression → baseline with \"minimal processing\"\n",
    "    - Ridge / Lasso → regularization check\n",
    "    - RandomForest / Boosting → complex models\n",
    "\n",
    "\n",
    "- check if we have a liner signal\n",
    "- figure out how features are adequate\n",
    "- expected RMSE ≈ 0.20–0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "\n",
    "linear_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', linear)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_linear = cross_val_score(\n",
    "    linear_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "linear_rmse = -scores_linear.mean()\n",
    "print('RMSE = ', linear_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "We got a very good score for the 2nd baseline - LinearRegression.\n",
    "\n",
    "The very good RMSE of LinearRegression indicates that there is a strong linear signal in the data.\n",
    "However, due to the presence of highly correlated features, the coefficients of the linear model may be unstable and hard to interpret.\n",
    "\n",
    "Multicollinearity does not necessarily worsen predictive performance, but it leads to unstable and non-interpretable coefficients in linear models. Therefore, highly correlated features are removed to obtain a stable and interpretable baseline.\n",
    "\n",
    "The goal was not to improve the score, but to obtain a cleaner and more interpretable baseline.\n",
    "This allows a fair comparison with more complex models and helps separate the effect of feature engineering from the effect of model complexity.\n",
    "\n",
    "We remove highly correlated features not to handicap linear models, but to obtain a stable and interpretable baseline that allows us to isolate the effect of model complexity.\n",
    "\n",
    "If we remove the correlations and look at the RMSE of the linear model:\n",
    "- It's easier to see how much the complex model actually improves the results.\n",
    "- It's easy to argue that \"Boosting gives +0.03 RMSE because the linear model gave 0.18, and the baseline was 0.22.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "Why remove highly correlated features for LinearRegression when creating a baseline:\n",
    "\n",
    "1. Coefficient Stability\n",
    "    - Highly correlated features → linear model coefficients are unstable\n",
    "    - With repeated splits (KFold), the coefficients \"jump\"\n",
    "    - Removing correlations makes the baseline predictable and stable\n",
    "2. Feature Interpretation\n",
    "    - Coefficients become meaningful: it's easy to understand which feature really influences the target\n",
    "    - Without correlated features, it's impossible to provide a correct explanation, for example, why increasing the area of ​​a house increases the price\n",
    "3. Fair Comparison with More Complex Models\n",
    "    - Removing redundant information so that the baseline doesn't receive an \"unnecessary bonus\" from duplicate features\n",
    "    - Allows you to accurately assess how much improvement a complex model (Boosting, RF, etc.) provides, and how much is simply due to good feature engineering\n",
    "4. Isolating the Effect of the Model\n",
    "    - Separating the Contribution of Data (Feature Engineering) and the Algorithm\n",
    "    - The RMSE of the clean baseline shows how much a simple linear model can provide\n",
    "    - Any improvement in a complex model now fairly reflects the advantage of the algorithm, not the features correlated data\n",
    "\n",
    "Conclusion:\n",
    "Highly correlated features are removed in the linear baseline not to worsen RMSE, but to obtain stable, interpretable coefficients and a fair comparison with more complex models. This allows us to isolate the contribution of model complexity from the effect of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "We should find the most correlated features to exclude them from model.\n",
    "\n",
    "There are 2 approaches how to do it:\n",
    "- feature–feature correlation\n",
    "- VIF (Variance Inflation Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "### Feature-feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = x_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "corr_matrix = x_train[numerical_features].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "'''\n",
    "corr_matrix =\n",
    "          A     B     C\n",
    "A       1.00  0.95  0.20\n",
    "B       0.95  1.00  0.10\n",
    "C       0.20  0.10  1.00\n",
    "\n",
    "np.ones(corr_matrix.shape) -> np.ones((3, 3))\n",
    "[[1. 1. 1.]\n",
    " [1. 1. 1.]\n",
    " [1. 1. 1.]]\n",
    "\n",
    "np.triu(np.ones((3, 3)), k=1)\n",
    "triu = upper triangular\n",
    "k=1 → start upper to diagonal\n",
    "diagonal and lower part = 0\n",
    "[[0. 1. 1.]\n",
    " [0. 0. 1.]\n",
    " [0. 0. 0.]]\n",
    "\n",
    ".astype(bool)\n",
    "[[False  True   True ]\n",
    " [False  False  True ]\n",
    " [False  False  False]]\n",
    "\n",
    "upper_tri = corr_matrix.where(mask)\n",
    "left values where mask = True\n",
    "others - NaN\n",
    "          A     B     C\n",
    "A       NaN   0.95  0.20\n",
    "B       NaN   NaN   0.10\n",
    "C       NaN   NaN   NaN\n",
    "'''\n",
    "\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "print(\"Highly correlated features to drop:\", to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "The correlation-based feature removal identified mostly deterministic or derived features (binary Has_* indicators, age-related variables, and quality proxies).\n",
    "This is expected and confirms that the algorithm works correctly.\n",
    "For the linear baseline, only one representative feature from each correlated group should be kept to ensure stability and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "Binary Has_* features were intentionally created to capture the presence effect of sparse continuous variables.\n",
    "However, for the linear baseline they are removed to avoid multicollinearity and ensure stable, interpretable coefficients.\n",
    "These features will be reintroduced in tree-based and final models, where such non-linear effects can be naturally exploited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "Binary Has_* features are not reintroduced into the final LinearRegression model, as they remain deterministic transformations of continuous variables and reintroduce multicollinearity.\n",
    "However, these features are included in regularized linear models and tree-based models, where they can improve predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "Sparse continuous features are kept in linear models, as zero values are valid and do not hinder linear estimation.\n",
    "Binary Has_* indicators are excluded to avoid deterministic multicollinearity and preserve coefficient interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Feature Filtering for Linear Regression Baseline (Grouped Analysis)\n",
    "\n",
    "To build a **stable and interpretable linear regression baseline**, we remove specific groups of features that introduce multicollinearity or redundant information.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟦 Group 1 — Quality / Condition Features (Ordinal)\n",
    "\n",
    "**Features:**\n",
    "\n",
    "* `GarageQual`\n",
    "* `GarageCond`\n",
    "* `PoolQC`\n",
    "\n",
    "**Why remove:**\n",
    "\n",
    "* These features are strongly correlated with:\n",
    "\n",
    "  * `OverallQual`\n",
    "  * `OverallCond`\n",
    "  * corresponding area-based features (`GarageArea`, `PoolArea`)\n",
    "* They encode similar information using different scales (ordinal vs numerical).\n",
    "* Including both causes multicollinearity and unstable coefficients.\n",
    "\n",
    "**Action:**\n",
    "\n",
    "* ❌ Drop `GarageQual`, `GarageCond`, `PoolQC`\n",
    "* ✅ Keep higher-level quality indicators like `OverallQual`\n",
    "\n",
    "---\n",
    "\n",
    "### 🟦 Group 2 — Age-Based Features\n",
    "\n",
    "**Features:**\n",
    "\n",
    "* `AgeAtSale`\n",
    "* `GarageAge`\n",
    "\n",
    "**Why remove:**\n",
    "\n",
    "* These are deterministic transformations of existing features:\n",
    "\n",
    "  * `AgeAtSale = YrSold - YearBuilt`\n",
    "  * `GarageAge = YrSold - GarageYrBlt`\n",
    "* Linear models do not benefit from having both representations.\n",
    "* This creates perfect or near-perfect multicollinearity.\n",
    "\n",
    "**Action:**\n",
    "\n",
    "* ❌ Drop `AgeAtSale`, `GarageAge`\n",
    "* ✅ Keep original year-based features (`YearBuilt`, `GarageYrBlt`)\n",
    "\n",
    "---\n",
    "\n",
    "### 🟦 Group 3 — Binary Indicators Derived from Continuous Features (`Has_*`)\n",
    "\n",
    "**Features:**\n",
    "\n",
    "* `Has_2ndFlrSF`\n",
    "* `Has_MasVnrArea`\n",
    "* `Has_ScreenPorch`\n",
    "* `Has_3SsnPorch`\n",
    "* `Has_WoodDeckSF`\n",
    "* `Has_MiscVal`\n",
    "* `Has_LowQualFinSF`\n",
    "* `Has_EnclosedPorch`\n",
    "* `Has_BsmtFinSF2`\n",
    "\n",
    "**Why remove:**\n",
    "\n",
    "* These features are **deterministically derived** from continuous variables.\n",
    "* Example:\n",
    "\n",
    "  * `MasVnrArea > 0 → Has_MasVnrArea = 1`\n",
    "* Including both causes:\n",
    "\n",
    "  * strong multicollinearity\n",
    "  * coefficient instability\n",
    "  * loss of interpretability (effect is split across features)\n",
    "\n",
    "**Important note:**\n",
    "\n",
    "* Sparse continuous features are **not a problem** for linear models.\n",
    "* Zero values are valid and simply indicate “absence”.\n",
    "\n",
    "**Action:**\n",
    "\n",
    "* ❌ Drop all `Has_*` features\n",
    "* ✅ Keep original continuous features:\n",
    "\n",
    "  * `MasVnrArea`\n",
    "  * `WoodDeckSF`\n",
    "  * `ScreenPorch`\n",
    "  * `3SsnPorch`\n",
    "  * etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 🟦 Group 4 — Highly Correlated Numerical Features (Feature–Feature)\n",
    "\n",
    "**Why remove:**\n",
    "\n",
    "* Some numerical features are strongly correlated with each other.\n",
    "* Linear models cannot reliably estimate separate effects in such cases.\n",
    "* This leads to:\n",
    "\n",
    "  * large coefficient variance\n",
    "  * unstable signs and magnitudes\n",
    "  * misleading interpretations\n",
    "\n",
    "**Action:**\n",
    "\n",
    "* ❌ Drop one feature from each highly correlated pair (based on correlation threshold)\n",
    "* ✅ Keep the more interpretable or fundamental feature\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Final Feature Filtering Summary\n",
    "\n",
    "| Feature Group       | Removed Features                     | Reason                                    |\n",
    "| ------------------- | ------------------------------------ | ----------------------------------------- |\n",
    "| Quality / Condition | `GarageQual`, `GarageCond`, `PoolQC` | Redundant with overall quality & area     |\n",
    "| Age-based           | `AgeAtSale`, `GarageAge`             | Deterministic transforms of year features |\n",
    "| Binary `Has_*`      | All `Has_*` indicators               | Deterministic, cause multicollinearity    |\n",
    "| Correlated numeric  | Selected correlated features         | Coefficient instability                   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Final Goal\n",
    "\n",
    "The purpose of this feature filtering is **not to maximize performance**, but to:\n",
    "\n",
    "* obtain stable and interpretable coefficients\n",
    "* build a clean and trustworthy linear baseline\n",
    "* enable fair comparison with more complex models\n",
    "* isolate the effect of model complexity from feature engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### VIF (Variance Inflation Factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X = x_train[numerical_features].copy()\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))\n",
    "# threshold = 5\n",
    "# while vif_data['VIF'].max() > threshold:\n",
    "#     drop_feature = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "#     X = X.drop(columns=[drop_feature])\n",
    "#     vif_data = pd.DataFrame()\n",
    "#     vif_data[\"feature\"] = X.columns\n",
    "#     vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "Extremely high or infinite VIF values indicate strong linear dependencies between features.\n",
    "Such features are not removed blindly; instead, we keep a single representative feature from each correlated group to obtain a stable and interpretable linear baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "### Remove correlated features for Linear Regrassion baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "#### inf ∞\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "TotalBath = FullBath + HalfBath x 0.5 + BsmtFullBath + BsmtHalfBath x 0.5\n",
    "\n",
    "The bath-related features form an exact linear combination.\n",
    "To avoid perfect multicollinearity, we keep the aggregated feature `TotalBath`\n",
    "and remove its individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_vif = x_train.drop(columns=['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'])\n",
    "to_drop = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "#### Very big VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {},
   "source": [
    "- GrLivArea: Above grade (ground) living area square feet\n",
    "- 1stFlrSF: First Floor square feet\n",
    "- 2ndFlrSF: Second floor square feet\n",
    "- TotalBsmtSF: Total square feet of basement area\n",
    "\n",
    "TotalSF:\n",
    "\n",
    "train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "\n",
    "Area-related features are strongly correlated.\n",
    "We keep a single aggregated feature `TotalSF` and remove its components\n",
    "to reduce multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_vif = x_train.drop(columns=['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'])\n",
    "to_drop += ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "GarageYrBlt:\n",
    "\n",
    "train['GarageAge'] = train['YrSold'] - train['GarageYrBlt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_vif = x_train.drop(columns=['GarageYrBlt'])\n",
    "to_drop += ['GarageYrBlt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "YearBuilt, YearRemodAdd:\n",
    "\n",
    "train['AgeAtSale'] = train['YrSold'] - train['YearBuilt']\n",
    "\n",
    "train['Remodeled'] = (train['YearRemodAdd'] != train['YearBuilt']).astype(int)\n",
    "\n",
    "train['IsNew'] = (train['YrSold'] == train['YearBuilt']).astype(int)\n",
    "\n",
    "train['Remodeled'] = (train['YearRemodAdd'] != train['YearBuilt']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_vif = x_train.drop(columns=['YearBuilt', 'YearRemodAdd'])\n",
    "to_drop += ['YearBuilt', 'YearRemodAdd']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "GarageCars vs GarageArea - I'll remove GarageArea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_vif = x_train.drop(columns=['GarageArea'])\n",
    "to_drop += ['GarageArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vif = x_train.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "numerical_features_vif = [f for f in numerical_features if f not in to_drop]\n",
    "X = x_train_vif[numerical_features_vif].copy()\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "We removed obvious correlations with the highest VIF. Next we will find other correlated features with a high VIF and remove them. This will remove obvious redundancies and obtain a stable and interpretable linear baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "#### Remove duplicates based on meaning, not VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "Living area\n",
    "\n",
    "- remain - GrLivArea\n",
    "- remove - TotRmsAbvGrd, BedroomAbvGr, KitchenAbvGr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['TotRmsAbvGrd', 'BedroomAbvGr', 'KitchenAbvGr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "Lot\n",
    "\n",
    "- remain - LotArea\n",
    "- remove - LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop += ['LotArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178",
   "metadata": {},
   "source": [
    "Quality\n",
    "\n",
    "- remain - OverallQual\n",
    "- remove - KitchenQual, ExterQual, GarageQual, GarageCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop += ['KitchenQual', 'ExterQual', 'GarageQual', 'GarageCond']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "Porch\n",
    "\n",
    "- remain - TotalPorchSF\n",
    "- remove - OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop += ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {},
   "source": [
    "Recalculate VIF after remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vif = x_train_vif.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "numerical_features_vif = [f for f in numerical_features_vif if f not in to_drop]\n",
    "X = x_train_vif[numerical_features_vif].copy()\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "After removing semantic duplicates (features describing the same concept),\n",
    "VIF values remain relatively high for some variables.\n",
    "\n",
    "This is expected for housing data, where size- and quality-related features\n",
    "are naturally correlated.\n",
    "\n",
    "The goal of this step was not to minimize VIF values, but to remove obvious\n",
    "redundancies and obtain a stable and interpretable linear baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186",
   "metadata": {},
   "source": [
    "### LinearRegression before removing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "\n",
    "linear_pipeline_full = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', linear)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_linear = cross_val_score(\n",
    "    linear_pipeline_full,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "linear_rmse = -scores_linear.mean()\n",
    "print('RMSE = ', linear_rmse)\n",
    "\n",
    "linear_pipeline_full.fit(x_train, y_train)\n",
    "\n",
    "feature_names = (\n",
    "    linear_pipeline_full\n",
    "    .named_steps['preprocessor']\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "\n",
    "coefs_full = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef_full': linear_pipeline_full.named_steps['model'].coef_\n",
    "})\n",
    "\n",
    "coefs_full['abs_coef_full'] = coefs_full['coef_full'].abs()\n",
    "coefs_full.sort_values('abs_coef_full', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "### LinearRegression after removing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler_candidates_vif = [\n",
    "    col for col in standard_scaler_candidates\n",
    "    if col in x_train_vif.columns\n",
    "]\n",
    "\n",
    "print(f\"Original scaler candidates: {len(standard_scaler_candidates)}\")\n",
    "print(f\"VIF scaler candidates: {len(standard_scaler_candidates_vif)}\")\n",
    "print(\"Removed from scaler:\",\n",
    "      set(standard_scaler_candidates) - set(standard_scaler_candidates_vif))\n",
    "\n",
    "preprocessor_vif = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), standard_scaler_candidates_vif)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "linear_pipeline_clean = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor_vif),\n",
    "        ('model', LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_linear = cross_val_score(\n",
    "    linear_pipeline_clean,\n",
    "    x_train_vif,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "linear_rmse = -scores_linear.mean()\n",
    "print('RMSE = ', linear_rmse)\n",
    "\n",
    "linear_pipeline_clean.fit(x_train_vif, y_train)\n",
    "\n",
    "feature_names_clean = (\n",
    "    linear_pipeline_clean\n",
    "    .named_steps['preprocessor']\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "\n",
    "coefs_clean = pd.DataFrame({\n",
    "    'feature': feature_names_clean,\n",
    "    'coef_clean': linear_pipeline_clean.named_steps['model'].coef_\n",
    "})\n",
    "\n",
    "coefs_clean['abs_coef_clean'] = coefs_clean['coef_clean'].abs()\n",
    "coefs_clean.sort_values('abs_coef_clean', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "After removing highly correlated and semantically duplicated features,\n",
    "the RMSE of the linear model increased only marginally\n",
    "(from 0.1206 to 0.1215).\n",
    "\n",
    "This confirms that multicollinearity had little impact on predictive performance,\n",
    "but strongly affected coefficient stability and interpretability.\n",
    "\n",
    "The cleaned feature set provides a more stable and interpretable linear baseline,\n",
    "while preserving almost all predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = coefs_full.merge(\n",
    "    coefs_clean,\n",
    "    on='feature',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "comparison['coef_diff'] = comparison['coef_clean'] - comparison['coef_full']\n",
    "comparison.sort_values('coef_diff', key=np.abs, ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {},
   "source": [
    "Comparing coefficients before and after removing correlated features\n",
    "shows substantial instability in the linear model.\n",
    "\n",
    "Several features changed their coefficients dramatically in both magnitude\n",
    "and sign, indicating that the original model relied on compensating effects\n",
    "between correlated predictors.\n",
    "\n",
    "After removing correlated features, the coefficients became more stable,\n",
    "smaller in magnitude, and more interpretable, with major effects concentrated\n",
    "in truly meaningful variables such as living area and overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in x_train.iterrows():\n",
    "#     print(f\"Row {index}: TotalBsmtSF={row['TotalBsmtSF']}, 1stFlrSF={row['1stFlrSF']}, 2ndFlrSF={row['2ndFlrSF']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {},
   "source": [
    "## Baseline №3 — Ridge / Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "#### Default model (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', ridge)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_ridge = cross_val_score(\n",
    "    ridge_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "ridge_rmse = -scores_ridge.mean()\n",
    "print('RMSE = ', ridge_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "#### Params adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 23\n",
    "ridge = Ridge(alpha=a)\n",
    "\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', ridge)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_ridge = cross_val_score(\n",
    "    ridge_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "ridge_rmse = -scores_ridge.mean()\n",
    "print(f'a = {a} RMSE = {ridge_rmse}')\n",
    "\n",
    "ridge_pipeline.fit(x_train, y_train)\n",
    "\n",
    "feature_names = (\n",
    "    ridge_pipeline\n",
    "    .named_steps['preprocessor']\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "\n",
    "coefs = ridge_pipeline.named_steps['model'].coef_\n",
    "\n",
    "coef_df_ridge = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef': coefs\n",
    "})\n",
    "\n",
    "coef_df_ridge['abs_coef'] = coef_df_ridge['coef'].abs()\n",
    "coef_df_ridge = coef_df_ridge.sort_values('abs_coef', ascending=False).head(300)\n",
    "coef_df_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "- a = 22 RMSE = 0.1130538533754319\n",
    "- a = 23 RMSE = 0.11305137568662804\n",
    "- a = 24 RMSE = 0.11305182827381646"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {},
   "source": [
    "#### Default model (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "lasso_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', lasso)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_lasso = cross_val_score(\n",
    "    lasso_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "lasso_rmse = -scores_lasso.mean()\n",
    "print('RMSE = ', lasso_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204",
   "metadata": {},
   "source": [
    "#### Params adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = [0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01]\n",
    "# alpha ↓  ⇒  max_iter ↑\n",
    "\n",
    "a = 0.00061\n",
    "mi = 6000\n",
    "\n",
    "lasso = Lasso(alpha=a, max_iter=mi)\n",
    "\n",
    "lasso_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', lasso)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_lasso = cross_val_score(\n",
    "    lasso_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "lasso_rmse = -scores_lasso.mean()\n",
    "print(f'a = {a}, max_iter = {mi}, RMSE = {lasso_rmse}')\n",
    "\n",
    "lasso_pipeline.fit(x_train, y_train)\n",
    "\n",
    "feature_names = (\n",
    "    lasso_pipeline\n",
    "    .named_steps['preprocessor']\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "\n",
    "coefs = lasso_pipeline.named_steps['model'].coef_\n",
    "\n",
    "coef_df_lasso = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coef': coefs\n",
    "})\n",
    "\n",
    "coef_df_lasso['abs_coef'] = coef_df_lasso['coef'].abs()\n",
    "coef_df_lasso = coef_df_lasso.sort_values('abs_coef', ascending=False).head(30)\n",
    "coef_df_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206",
   "metadata": {},
   "source": [
    "- a = 0.0003, max_iter = 10000, RMSE = 0.11417273190164605\n",
    "- a = 0.0005, max_iter = 10000, RMSE = 0.11302415669046681\n",
    "- a = 0.0006, max_iter = 10000, RMSE = 0.11297160574224321\n",
    "- a = 0.00061, max_iter = 6000, RMSE = 0.1129705315617295\n",
    "- a = 0.00062, max_iter = 10000, RMSE = 0.11297215846765736\n",
    "- a = 0.00065, max_iter = 10000, RMSE = 0.11299721819376218\n",
    "- a = 0.0007, max_iter = 10000, RMSE = 0.11306570736956387\n",
    "- a = 0.001, max_iter = 10000, RMSE = 0.11374769132124171"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207",
   "metadata": {},
   "source": [
    "### Compare Ridge vs Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare = (\n",
    "    coef_df_ridge[['feature', 'coef']]\n",
    "    .rename(columns={'coef': 'ridge_coef'})\n",
    "    .merge(\n",
    "        coef_df_lasso[['feature', 'coef']]\n",
    "        .rename(columns={'coef': 'lasso_coef'}),\n",
    "        on='feature',\n",
    "        how='outer'\n",
    "    )\n",
    ")\n",
    "coef_compare['abs_ridge'] = coef_compare['ridge_coef'].abs()\n",
    "coef_compare['abs_lasso'] = coef_compare['lasso_coef'].abs()\n",
    "\n",
    "coef_compare['lasso_zero'] = coef_compare['lasso_coef'].fillna(0) == 0\n",
    "coef_compare['ridge_zero'] = coef_compare['ridge_coef'].fillna(0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209",
   "metadata": {},
   "source": [
    "#### Lasso sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare.sort_values(\n",
    "    by='abs_lasso',\n",
    "    ascending=False\n",
    ").head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211",
   "metadata": {},
   "source": [
    "#### Ridge sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare.sort_values(\n",
    "    by='abs_ridge',\n",
    "    ascending=False\n",
    ").head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {},
   "source": [
    "#### Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214",
   "metadata": {},
   "source": [
    "- Ridge considers important\n",
    "- Lasso dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare[\n",
    "    (coef_compare['lasso_zero']) &\n",
    "    (coef_compare['abs_ridge'] > 0.01)\n",
    "].sort_values('abs_ridge', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216",
   "metadata": {},
   "source": [
    "- Both consider important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare[\n",
    "    (~coef_compare['lasso_zero']) &\n",
    "    (coef_compare['abs_ridge'] > 0.01)\n",
    "].sort_values('abs_lasso', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218",
   "metadata": {},
   "source": [
    "#### Sign of coeffs sanity-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare[\n",
    "    coef_compare['lasso_coef'] * coef_compare['ridge_coef'] < 0\n",
    "].sort_values('abs_lasso', ascending=False).head(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {},
   "source": [
    "#### Coeffs correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare[['ridge_coef', 'lasso_coef']].dropna().corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222",
   "metadata": {},
   "source": [
    "The Ridge and Lasso coefficients have a high correlation (~0.83), indicating agreement between the models in assessing the direction and relative importance of features. However, Lasso performs additional feature selection by setting some coefficients to zero, leading to discrepancies in the weight distribution while maintaining the overall influence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "(coef_compare['lasso_zero']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225",
   "metadata": {},
   "source": [
    "#### Default model (alpha = 1, l1_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic = ElasticNet(\n",
    "    random_state=42,\n",
    "    max_iter=10_000\n",
    ")\n",
    "\n",
    "elastic_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', elastic)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_elastic = cross_val_score(\n",
    "    elastic_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "elastic_rmse = -scores_elastic.mean()\n",
    "print('RMSE = ', elastic_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {},
   "source": [
    "#### Params adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228",
   "metadata": {},
   "source": [
    "##### Find with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1)\n",
    "# param_grid = {\n",
    "#     \"model__alpha\": [0.01, 0.1, 1, 10],\n",
    "#     \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# }\n",
    "# {'model__alpha': 0.01, 'model__l1_ratio': 0.1}\n",
    "# 0.11448261056707518\n",
    "\n",
    "# 2) \n",
    "# param_grid = {\n",
    "#     \"model__alpha\": [0.001, 0.01, 0.1],\n",
    "#     \"model__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "# }\n",
    "# {'model__alpha': 0.001, 'model__l1_ratio': 0.5}\n",
    "# 0.11300705467782828\n",
    "\n",
    "# 3)\n",
    "# param_grid = {\n",
    "#     \"model__alpha\": [0.0001, 0.001, 0.01],\n",
    "#     \"model__l1_ratio\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# }\n",
    "# {'model__alpha': 0.001, 'model__l1_ratio': 0.5}\n",
    "# 0.11300705467782828\n",
    "\n",
    "# 4)\n",
    "# param_grid = {\n",
    "#     \"model__alpha\": [0.0005, 0.001, 0.002, 0.003],\n",
    "#     \"model__l1_ratio\": [0.4, 0.5, 0.6]\n",
    "# }\n",
    "# {'model__alpha': 0.001, 'model__l1_ratio': 0.6}\n",
    "# 0.11298613953092669\n",
    "\n",
    "# param_grid = {\n",
    "#     \"model__alpha\": [0.0009, 0.001, 0.0011],\n",
    "#     \"model__l1_ratio\": [0.53, 0.54, 0.55]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(\n",
    "#     elastic_pipeline,\n",
    "#     param_grid,\n",
    "#     cv=kf,\n",
    "#     scoring=\"neg_root_mean_squared_error\",\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid.fit(x_train, y_train)\n",
    "\n",
    "# best_rmse = -grid.best_score_\n",
    "# print(f'params: {grid.best_params_} RMSE = {best_rmse}')\n",
    "\n",
    "# best_model = grid.best_estimator_\n",
    "# coefs = best_model.named_steps[\"model\"].coef_\n",
    "# feature_names = best_model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# coef_df = pd.DataFrame({\n",
    "#     \"feature\": feature_names,\n",
    "#     \"coef\": coefs\n",
    "# }).sort_values(by=\"coef\", key=np.abs, ascending=False)\n",
    "\n",
    "# coef_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230",
   "metadata": {},
   "source": [
    "Best solution: params: {'model__alpha': 0.001, 'model__l1_ratio': 0.54} RMSE = 0.11297908867113624"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231",
   "metadata": {},
   "source": [
    "##### Apply optimal params after GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.001\n",
    "l1_r = 0.54\n",
    "mi = 10_000\n",
    "\n",
    "elastic = ElasticNet(\n",
    "    alpha=a,\n",
    "    l1_ratio=l1_r,\n",
    "    random_state=42,\n",
    "    max_iter=mi\n",
    ")\n",
    "\n",
    "elastic_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', elastic)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_elastic = cross_val_score(\n",
    "    elastic_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "elastic_rmse = -scores_elastic.mean()\n",
    "print(f'alpha = {a}, l1_ratio = {l1_r}, max_iter = {mi}, RMSE = {elastic_rmse}')\n",
    "\n",
    "elastic_pipeline.fit(x_train, y_train)\n",
    "\n",
    "feature_names = (\n",
    "    elastic_pipeline\n",
    "    .named_steps['preprocessor']\n",
    "    .get_feature_names_out()\n",
    ")\n",
    "\n",
    "coefs = elastic_pipeline.named_steps['model'].coef_\n",
    "\n",
    "coef_df_elastic = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": coefs\n",
    "}).sort_values(by=\"coef\", key=np.abs, ascending=False)\n",
    "\n",
    "coef_df_elastic.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233",
   "metadata": {},
   "source": [
    "### Compare Ridge vs Lasso vs ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare = (\n",
    "    coef_df_ridge[['feature', 'coef']]\n",
    "    .rename(columns={'coef': 'ridge_coef'})\n",
    "    .merge(\n",
    "        coef_df_lasso[['feature', 'coef']]\n",
    "        .rename(columns={'coef': 'lasso_coef'}),\n",
    "        on='feature',\n",
    "        how='outer'\n",
    "    )\n",
    "    .merge(\n",
    "        coef_df_elastic[['feature', 'coef']]\n",
    "        .rename(columns={'coef': 'elastic_coef'}),\n",
    "        on='feature',\n",
    "        how='outer'\n",
    "    )\n",
    ")\n",
    "\n",
    "for col in ['ridge_coef', 'lasso_coef', 'elastic_coef']:\n",
    "    coef_compare[f'abs_{col}'] = coef_compare[col].abs()\n",
    "\n",
    "coef_compare['lasso_zero'] = coef_compare['lasso_coef'].fillna(0) == 0\n",
    "coef_compare['elastic_zero'] = coef_compare['elastic_coef'].fillna(0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235",
   "metadata": {},
   "source": [
    "#### ElasticNet sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare.sort_values(\n",
    "    by='abs_elastic_coef',\n",
    "    ascending=False\n",
    ").head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare['stable_feature'] = (\n",
    "    (~coef_compare['lasso_zero']) &\n",
    "    (~coef_compare['elastic_zero'])\n",
    ")\n",
    "\n",
    "coef_compare['lasso_only_killed'] = (\n",
    "    coef_compare['lasso_zero'] &\n",
    "    (~coef_compare['elastic_zero'])\n",
    ")\n",
    "\n",
    "coef_compare['elastic_killed'] = (\n",
    "    coef_compare['elastic_zero']\n",
    ")\n",
    "\n",
    "coef_compare.sort_values(\n",
    "    by='abs_elastic_coef',\n",
    "    ascending=False\n",
    ").head(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238",
   "metadata": {},
   "source": [
    "ElasticNet performed better because the data contained\n",
    "groups of multicollinear features (especially one-hot categories).\n",
    "Lasso loses some signal due to its strict feature selection,\n",
    "Ridge preserves noise. ElasticNet preserves informative groups\n",
    "and suppresses irrelevant features, ensuring an optimal bias–variance balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239",
   "metadata": {},
   "source": [
    "#### How many features removed Lasso vs ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare[['lasso_zero', 'elastic_zero']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare['abs_elastic_coef'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242",
   "metadata": {},
   "source": [
    "ElasticNet with a selected alpha operates in a soft\n",
    "regularization mode: instead of harsh feature removal, the model\n",
    "preserves all features but suppresses noise through shrinkage.\n",
    "This results in better generalization compared to\n",
    "Lasso, which loses some of the useful signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243",
   "metadata": {},
   "source": [
    "## Baseline №4 — RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244",
   "metadata": {},
   "source": [
    "Before further optimizing linear models, we evaluate a simple non-linear model to check whether performance is limited by linearity rather than feature selection.\n",
    "If a non-linear model significantly outperforms ElasticNet, this indicates that increasing model expressiveness is more impactful than refining linear features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245",
   "metadata": {},
   "source": [
    "### Default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', rf)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_rf = cross_val_score(\n",
    "    rf_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "rf_rmse = -scores_rf.mean()\n",
    "print('RMSE = ', rf_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247",
   "metadata": {},
   "source": [
    "### Params adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features=0.5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', rf)\n",
    "    ]\n",
    ")\n",
    "\n",
    "scores_rf = cross_val_score(\n",
    "    rf_pipeline,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "rf_rmse = -scores_rf.mean()\n",
    "print('CV RMSE = ', rf_rmse)\n",
    "\n",
    "rf_pipeline.fit(x_train, y_train)\n",
    "rf_oob_rmse = ((y_train - rf_pipeline.named_steps['model'].oob_prediction_)**2).mean()**0.5\n",
    "print('OOB RMSE =', rf_oob_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249",
   "metadata": {},
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "CV RMSE =  0.1349470584607141\n",
    "OOB RMSE = 0.13401937753564025\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features=0.5,\n",
    "    random_state=42,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "CV RMSE =  0.13151503991067154\n",
    "OOB RMSE = 0.1305056553066029\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    max_features=0.5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "CV RMSE =  0.13147013124637516\n",
    "OOB RMSE = 0.13076976447669397\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250",
   "metadata": {},
   "source": [
    "After evaluating multiple baseline models, ElasticNet significantly outperformed RandomForest, achieving an RMSE of 0.113 compared to 0.131 for the best-tuned RandomForest.\n",
    "This indicates that the target variable is largely driven by additive linear effects, and that extensive feature engineering successfully captured the underlying structure of the data.\n",
    "Manual tuning of RandomForest, including adjusting max_features and min_samples_leaf, yielded only a modest improvement, suggesting diminishing returns for tree-based bagging methods on this dataset.\n",
    "Therefore, the next step is to explore boosting-based models that can capture both linear and non-linear effects, such as LightGBM or CatBoost, to further reduce RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251",
   "metadata": {},
   "source": [
    "## Compare Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252",
   "metadata": {},
   "source": [
    "| Model      | CV RMSE | OOB RMSE / notes |\n",
    "| ---------- | ------- | ---------------- |\n",
    "| Dummy      | 0.39722383480557066    | –                |\n",
    "| Linear     | 0.1215493471393804    | –                |\n",
    "| Ridge      | 0.11305137568662811    | –                |\n",
    "| Lasso      | 0.1129705315617295    | –                |\n",
    "| ElasticNet | 0.11297908867113624   | –                |\n",
    "| RF tuned   | 0.13147013124637513   | OOB 0.13076976447669397       |\n",
    "\n",
    "Evaluated several baseline models using 5-fold cross-validation with RMSE as the primary metric.\n",
    "Linear models with regularization (Ridge, Lasso, ElasticNet) achieved the best performance with CV RMSE around 0.113, significantly outperforming both the Dummy baseline and RandomForest.\n",
    "This indicates that the target variable is largely driven by linear additive relationships between engineered features, and that regularization effectively handles feature redundancy introduced by one-hot encoding.\n",
    "RandomForest did not provide additional gains, likely due to high feature sparsity and limited nonlinear signal.\n",
    "\n",
    "Tree-based models such as RandomForest do not extrapolate beyond the range of values observed during training, since predictions in each leaf are based on averaging target values seen in that region of the feature space.\n",
    "In contrast, linear models are able to extrapolate learned trends, which is especially important for features with strong monotonic relationships (e.g. living area or overall quality).\n",
    "In the House Prices dataset, the target variable is largely driven by such monotonic, approximately linear effects (in log-space), which explains why regularized linear models outperform RandomForest in terms of RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253",
   "metadata": {},
   "source": [
    "## Let's find the most correlated features with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = x_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = x_train.select_dtypes(include=['object']).columns\n",
    "boolean_features = x_train.select_dtypes(include=['boolean']).columns\n",
    "\n",
    "print(\"numerical_features:\", numerical_features)\n",
    "print(\"categorical_features:\", categorical_features)\n",
    "print(\"boolean_features:\", boolean_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of numerical features with SalePrice\n",
    "correlations = x_train[numerical_features].copy()\n",
    "correlations['SalePrice'] = y_train\n",
    "corr_matrix = correlations.corr()\n",
    "\n",
    "# Sort by absolute correlation with SalePrice\n",
    "top_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False)\n",
    "print(top_corr.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_features = top_corr.index[1:11]  # exclude SalePrice\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    sns.scatterplot(x=x_train[feature], y=y_train)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
